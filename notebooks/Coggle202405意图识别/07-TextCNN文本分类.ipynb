{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c856aa61-938b-422e-bd10-a8f86e5b9dc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import jieba\n",
    "import pandas as pd\n",
    "\n",
    "data_dir = 'https://mirror.coggle.club/dataset/coggle-competition/'\n",
    "train_data = pd.read_csv(data_dir + 'intent-classify/train.csv', sep='\\t', header=None)\n",
    "test_data = pd.read_csv(data_dir + 'intent-classify/test.csv', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a0d4a0d-03fc-4a54-92a9-cd87f94b1170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.2+cu121'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e599690-b330-437e-896f-c54f8de97601",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>还有双鸭山到淮阴的汽车票吗13号的</td>\n",
       "      <td>Travel-Query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>从这里怎么回家</td>\n",
       "      <td>Travel-Query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>随便播放一首专辑阁楼里的佛里的歌</td>\n",
       "      <td>Music-Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>给看一下墓王之王嘛</td>\n",
       "      <td>FilmTele-Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>我想看挑战两把s686打突变团竞的游戏视频</td>\n",
       "      <td>Video-Play</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0              1\n",
       "0      还有双鸭山到淮阴的汽车票吗13号的   Travel-Query\n",
       "1                从这里怎么回家   Travel-Query\n",
       "2       随便播放一首专辑阁楼里的佛里的歌     Music-Play\n",
       "3              给看一下墓王之王嘛  FilmTele-Play\n",
       "4  我想看挑战两把s686打突变团竞的游戏视频     Video-Play"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8b7c1b6-4730-4bc9-8637-51c671107e6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = train_data.sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6232746-8cf6-4766-b6e9-b8763c938f90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data[1], lbl = pd.factorize(train_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9d63c0c-0e7d-4f57-9a6f-6fd1c23c5642",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def coustom_data_iter(texts, labels):\n",
    "    for x, y in zip(texts, labels):\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cdf3cc7-4b70-4b21-923e-65e7c2af5dac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_iter = coustom_data_iter(train_data[0].values[:], train_data[1].values[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ab5ebd1-f28d-4c46-a28e-57d415454c24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.812 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = jieba.lcut\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text, _ in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36956a32-1053-4639-a543-cd6c09fbdd2c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '的', '我', '一下', '播放', '是', '吗', '给', '帮', '一个']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get_itos()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48580411-289b-4ef8-a4e5-a35040fe2014",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 41]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['我', '一下', '今天'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b80b35e3-0f63-40a1-9813-78a5113be18e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_pipeline(x): return vocab(tokenizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7683e758-2d0f-44ec-b561-212c1fe91829",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed_text = torch.tensor(text_pipeline('今天我们在这里'), dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "779340b0-5f93-4054-9752-481392e375a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c49f4b3-bf96-430a-8856-d61e0867cff9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def collate_batch(batch, max_len=40):\n",
    "    label_list, text_list = [], []\n",
    "    for (_text, _label) in batch:\n",
    "        label_list.append(_label)\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        processed_text = F.pad(processed_text, pad=[0, max_len,], mode='constant', value=0)\n",
    "        if len(processed_text) > max_len:\n",
    "            processed_text = processed_text[:max_len]\n",
    "\n",
    "        text_list.append(processed_text)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = pad_sequence(text_list).T\n",
    "    return label_list.to(device), text_list.to(device)\n",
    "\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "train_iter = to_map_style_dataset(train_iter)\n",
    "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76c856c5-cb3a-4c39-b063-9f7b7067caaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "#%% Text CNN model\n",
    "class textCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_dim, kernel_wins, num_class=12):\n",
    "        super(textCNN, self).__init__()\n",
    "        #load pretrained embedding in embedding layer.\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim)\n",
    "        # self.embed.weight.data.copy_(torch.from_numpy(pretrained_w2v))\n",
    "        \n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, emb_dim, (w, emb_dim)) for w in kernel_wins])\n",
    "        #Dropout layer\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "        \n",
    "        #FC layer\n",
    "        self.fc = nn.Linear(len(kernel_wins)*emb_dim, num_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # torch.Size([16, 40])\n",
    "        # print(x.shape)\n",
    "        emb_x = self.embed(x)\n",
    "        \n",
    "        # torch.Size([16, 40, 100])\n",
    "        # print(emb_x.shape)\n",
    "        \n",
    "        # batch size * channel * height * width\n",
    "        # batch size * channel * word count * embedding\n",
    "        # torch.Size([16, 1, 40, 100])\n",
    "        emb_x = emb_x.unsqueeze(1)\n",
    "        # print(emb_x.shape)\n",
    "\n",
    "        con_x = [conv(emb_x) for conv in self.convs]\n",
    "\n",
    "        pool_x = [F.max_pool1d(x.squeeze(-1), x.size()[2]) for x in con_x]\n",
    "        fc_x = torch.cat(pool_x, dim=1)\n",
    "        # torch.Size([16, 300, 1])\n",
    "        # print(fc_x.shape)\n",
    "        fc_x = fc_x.squeeze(-1)\n",
    "\n",
    "        fc_x = self.dropout(fc_x)\n",
    "        logit = self.fc(fc_x)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7cb48f6-d567-4b57-b74d-2830cc2b902e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    for idx, (label, text) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text) in enumerate(dataloader):\n",
    "            predicted_label = model(text)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66f6580a-d4e2-4a6f-bc97-de95c5a139c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_class = len(lbl)\n",
    "vocab_size = len(vocab)\n",
    "emsize = 100\n",
    "model = textCNN(vocab_size, emsize, [3, 4 , 5], num_class).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3fca9b3-9a4f-4057-a8d4-18b5ee1dd1c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "# Hyperparameters\n",
    "EPOCHS = 20  # epoch\n",
    "LR = 0.001 # learning rate\n",
    "BATCH_SIZE = 16  # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 5.0, gamma=0.75)\n",
    "total_accu = None\n",
    "\n",
    "train_iter = coustom_data_iter(train_data[0].values[:], train_data[1].values[:])\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "\n",
    "num_train = int(len(train_dataset) * 0.75)\n",
    "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fee2746-9c22-43ea-a0b1-3a98f71ecc81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    \n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413f0821-4d58-4b9f-8b22-aee6ef3ceb41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_iter = coustom_data_iter(test_data[0].values[:], [0] * len(test_data))\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167373d5-902d-4081-84ba-8287b00548aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    test_pred = []\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text) in enumerate(dataloader):\n",
    "            predicted_label = model(text).argmax(1)\n",
    "            test_pred += list(predicted_label.cpu().numpy())\n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74bf13e-566f-4daa-b4b4-fab2b563a8e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pred = predict(test_dataloader)\n",
    "test_pred = [lbl[x] for x in test_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3499669d-3348-4a5b-9087-62b524cac89b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'ID': range(1, len(test_pred) + 1),\n",
    "    'Target': test_pred,\n",
    "}).to_csv('nlp_submit.csv', index=None)\n",
    "\n",
    "# 提交一下吧~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deef3ccf-c1de-4b5e-a10b-4a50c1db67b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "py3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
